import torch
import os
import json
from google import genai 
import argparse
import yaml
from dotenv import load_dotenv

# ANSI escape codes for colors
PINK = "\033[95m"
CYAN = "\033[96m"
YELLOW = "\033[93m"
NEON_GREEN = "\033[92m"
RESET_COLOR = "\033[0m"

# Load environment variables
load_dotenv()


def load_config(config_file):
    print("Loading configuration...")
    try:
        with open(config_file, "r") as file:
            return yaml.safe_load(file)
    except FileNotFoundError:
        print(f"Configuration file '{config_file}' not found.")
        exit(1)


def open_file(filepath):
    print("Opening file...")
    try:
        with open(filepath, "r", encoding="utf-8") as infile:
            return infile.read()
    except FileNotFoundError:
        print(f"File '{filepath}' not found.")
        return None


def load_or_generate_embeddings(vault_content, embeddings_file, client):
    # This function should only load embeddings since they are generated by generate_embeddings.py
    if os.path.exists(embeddings_file):
        print(f"Loading embeddings from '{embeddings_file}'...")
        try:
            with open(embeddings_file, "r", encoding="utf-8") as file:
                # Ensure the loaded data is a list before converting to tensor
                loaded_data = json.load(file)
                if isinstance(loaded_data, list):
                    return torch.tensor(loaded_data)
                else:
                    print(f"Invalid data type in embeddings file '{embeddings_file}'.")
        except json.JSONDecodeError:
            print(f"Invalid JSON format in embeddings file '{embeddings_file}'.")
    
    # If file is missing or invalid, initialize empty tensor
    print("WARNING: Embeddings file missing or invalid. Rerunning generate_embeddings.py is recommended.")
    return torch.empty(0)


# NOTE: The generate_embeddings and save_embeddings functions are typically NOT in emailrag.py, 
# but since they are here, we ensure they use the correct Gemini syntax.

def generate_embeddings(vault_content, client):
    print("Generating embeddings...")
    embeddings = []
    EMBEDDING_MODEL = "text-embedding-004" 
    
    for content in vault_content:
        if content.strip(): 
            try:
                response = client.models.embed_content(
                    model=EMBEDDING_MODEL, contents=content 
                )
                # --- CRITICAL FIX 1: ACCESS EMBEDDING VECTOR ---
                embeddings.append(response.embeddings[0]) 
            except Exception as e:
                print(f"Error generating embeddings for content: {content[:30]}... Error: {str(e)}")
    return embeddings


def save_embeddings(embeddings, embeddings_file):
    print(f"Saving embeddings to '{embeddings_file}'...")
    try:
        # Convert tensor to list for JSON serialization if it's not already
        if isinstance(embeddings, torch.Tensor):
            embeddings = embeddings.tolist()
            
        with open(embeddings_file, "w", encoding="utf-8") as file:
            json.dump(embeddings, file)
    except Exception as e:
        print(f"Error saving embeddings: {str(e)}")


def get_relevant_context(
    rewritten_input, vault_embeddings, vault_content, top_k, client
):
    print("Retrieving relevant context...")
    if vault_embeddings.nelement() == 0:
        return []
    
    EMBEDDING_MODEL = "text-embedding-004"
    
    try:
        response = client.models.embed_content(
            model=EMBEDDING_MODEL, contents=rewritten_input 
        )
        # --- CRITICAL FIX 2: ACCESS EMBEDDING VECTOR ---
        input_embedding = response.embeddings[0]
        
        # Ensure input_embedding is a tensor before calling torch.cosine_similarity
        input_embedding_tensor = torch.tensor(input_embedding).unsqueeze(0)
        
        cos_scores = torch.cosine_similarity(
            input_embedding_tensor, vault_embeddings
        )
        top_k = min(top_k, len(cos_scores))
        top_indices = torch.topk(cos_scores, k=top_k)[1].tolist()
        return [vault_content[idx].strip() for idx in top_indices]
    except Exception as e:
        print(f"Error getting relevant context: {str(e)}")
        return []


def chat_with_gpt(
    user_input,
    system_message,
    vault_embeddings,
    vault_content,
    model, 
    conversation_history,
    top_k,
    client,
):
    relevant_context = get_relevant_context(
        user_input, vault_embeddings, vault_content, top_k, client
    )
    if relevant_context:
        context_str = "\n".join(relevant_context)
        print("Context Pulled from Documents: \n\n" + CYAN + context_str + RESET_COLOR)
    else:
        print("No relevant context found.")

    user_input_with_context = user_input
    if relevant_context:
        user_input_with_context = f"Context:\n{context_str}\n\nQuestion: {user_input}"

    conversation_history.append({"role": "user", "content": user_input_with_context})
    messages = [{"role": "system", "content": system_message}, *conversation_history]

    try:
        # Concatenate history and system message into one prompt for simplicity
        full_prompt = f"{system_message}\n\nChat History:\n{conversation_history[:-1]}\n\n{user_input_with_context}"

        response = client.models.generate_content(
            model=model,
            contents=full_prompt,
            config={"temperature": 0.7, "max_output_tokens": 1000}
        )
        
        response_text = response.text
        
        return response_text
    except Exception as e:
        print(f"Error in chat completion: {str(e)}")
        return "An error occurred while processing your request."


def main():
    parser = argparse.ArgumentParser(description="Email RAG System")
    parser.add_argument(
        "--config", default="config.yaml", help="Path to the configuration file"
    )
    parser.add_argument(
        "--clear-cache", action="store_true", help="Clear the embeddings cache"
    )
    # --- FIXED: Change Default Model to Gemini model ---
    parser.add_argument("--model", default="gemini-2.5-flash", help="Gemini model to use")

    args = parser.parse_args()
    config = load_config(args.config)

    if args.clear_cache and os.path.exists(config["embeddings_file"]):
        print(f"Clearing embeddings cache at '{config['embeddings_file']}'...")
        os.remove(config["embeddings_file"])

    if args.model:
        # Ensure model is saved to a known path in config
        config.setdefault("gemini", {})
        config["gemini"]["model"] = args.model
    
    # --- FIXED: Initialize client using the official Gemini library ---
    client = genai.Client()

    vault_content = []
    if os.path.exists(config["vault_file"]):
        print(f"Loading content from vault '{config["vault_file"]}'...")
        with open(config["vault_file"], "r", encoding="utf-8") as vault_file:
            vault_content = vault_file.readlines()

    vault_embeddings_tensor = load_or_generate_embeddings(
        vault_content, config["embeddings_file"], client
    )

    conversation_history = []
    system_message = config["system_message"]

    print(PINK + "\nWelcome to the Email RAG System!" + RESET_COLOR)
    print(CYAN + "Type 'quit' to exit the chat." + RESET_COLOR)

    while True:
        user_input = input(
            YELLOW + "\nAsk a question about your emails: " + RESET_COLOR
        )
        if user_input.lower() == "quit":
            break

        try:
            # --- FIXED: Get the correct model name ---
            model_to_use = config.get("gemini", {}).get("model") or "gemini-2.5-flash"

            response = chat_with_gpt(
                user_input,
                system_message,
                vault_embeddings_tensor,
                vault_content,
                model_to_use, 
                conversation_history,
                config["top_k"],
                client,
            )
            print(NEON_GREEN + "Response: \n\n" + response + RESET_COLOR)
        except Exception as e:
            print(f"An error occurred: {str(e)}")


if __name__ == "__main__":
    main()